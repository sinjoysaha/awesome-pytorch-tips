{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tensors Directly on the Target device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Total time: 0.982s\n"
     ]
    }
   ],
   "source": [
    "# Creating on CPU -> copying to GPU\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    cpu_tensor = torch.ones((1000, 64, 64))\n",
    "    gpu_tensor = cpu_tensor.cuda()\n",
    "\n",
    "print(gpu_tensor.device)\n",
    "print(f\"Total time: {time.time() - start_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Total time: 0.101s\n"
     ]
    }
   ],
   "source": [
    "# Creating directly on GPU\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    gpu_tensor = torch.ones((1000, 64, 64), device='cuda')\n",
    "\n",
    "print(gpu_tensor.device)\n",
    "print(f\"Total time: {time.time() - start_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Sequential Layers when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without Sequential \n",
    "class ExampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        input_size = 2\n",
    "        output_size = 3\n",
    "        hidden_size = 16\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.input_activation = nn.ReLU()\n",
    "\n",
    "        self.mid_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mid_activation = nn.ReLU()\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.input_layer(x)\n",
    "        z = self.input_activation(z)\n",
    "\n",
    "        z = self.mid_layer(z)\n",
    "        z = self.mid_activation(z)\n",
    "\n",
    "        out = self.output_layer(z)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExampleModel(\n",
      "  (input_layer): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (input_activation): ReLU()\n",
      "  (mid_layer): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (mid_activation): ReLU()\n",
      "  (output_layer): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Output Shape: torch.Size([100, 3])\n"
     ]
    }
   ],
   "source": [
    "example_model = ExampleModel()\n",
    "print(example_model)\n",
    "print(f\"Output Shape: {example_model(torch.ones([100, 2])).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Sequential\n",
    "class ExampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        input_size = 2\n",
    "        output_size = 3\n",
    "        hidden_size = 16\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExampleModel(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "Output Shape: torch.Size([100, 3])\n"
     ]
    }
   ],
   "source": [
    "example_model = ExampleModel()\n",
    "print(example_model)\n",
    "print(f\"Output Shape: {example_model(torch.ones([100, 2])).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't Make Lists of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BadListModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        input_size = 2\n",
    "        output_size = 3\n",
    "        hidden_size = 16\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.input_activation = nn.ReLU()\n",
    "\n",
    "        # Generally used in residual layers\n",
    "        self.mid_layers = []\n",
    "        for _ in range(5):\n",
    "            self.mid_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.mid_layers.append(nn.ReLU())\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.input_layer(x)\n",
    "        z = self.input_activation(z)\n",
    "\n",
    "        for layer in self.mid_layers:\n",
    "            z = layer(z)\n",
    "        \n",
    "        out = self.output_layer(z)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BadListModel(\n",
      "  (input_layer): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (input_activation): ReLU()\n",
      "  (output_layer): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Output Shape: torch.Size([100, 3])\n"
     ]
    }
   ],
   "source": [
    "example_model = BadListModel()\n",
    "print(example_model)\n",
    "print(f\"Output Shape: {example_model(torch.ones([100, 2])).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks fine but `ERROR` occurs when moving model and input to GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m gpu_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones([\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m gpu_example_model \u001b[38;5;241m=\u001b[39m example_model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mgpu_example_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpu_input\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[11], line 25\u001b[0m, in \u001b[0;36mBadListModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_activation(z)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_layers:\n\u001b[1;32m---> 25\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(z)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "gpu_input = torch.ones([100, 2], device='cuda')\n",
    "gpu_example_model = example_model.cuda()\n",
    "print(f\"Output Shape: {gpu_example_model(gpu_input).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ERROR` occurs because the internal hidden layers in the list are not tracked and thus not moved to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectListModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        input_size = 2\n",
    "        output_size = 3\n",
    "        hidden_size = 16\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.input_activation = nn.ReLU()\n",
    "\n",
    "        # Generally used in residual layers\n",
    "        self.mid_layers = []\n",
    "        for _ in range(5):\n",
    "            self.mid_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.mid_layers.append(nn.ReLU())\n",
    "\n",
    "        # pass the list through a Sequential layer to keep track and move to GPU automatically\n",
    "        self.mid_layers = nn.Sequential(*self.mid_layers)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.input_layer(x)\n",
    "        z = self.input_activation(z)\n",
    "\n",
    "        for layer in self.mid_layers:\n",
    "            z = layer(z)\n",
    "        \n",
    "        out = self.output_layer(z)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CorrectListModel(\n",
      "  (input_layer): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (input_activation): ReLU()\n",
      "  (mid_layers): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (9): ReLU()\n",
      "  )\n",
      "  (output_layer): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Output Shape: torch.Size([100, 3])\n",
      "Output Shape: torch.Size([100, 3])\n"
     ]
    }
   ],
   "source": [
    "example_model = CorrectListModel()\n",
    "print(example_model)\n",
    "print(f\"Output Shape: {example_model(torch.ones([100, 2])).shape}\")\n",
    "\n",
    "gpu_input = torch.ones([100, 2], device='cuda')\n",
    "gpu_example_model = example_model.cuda()\n",
    "print(f\"Output Shape: {gpu_example_model(gpu_input).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Use of Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1695,  0.1188, -0.1798],\n",
      "        [ 0.1945,  0.1045, -0.1553],\n",
      "        [ 0.1858,  0.1026, -0.1594],\n",
      "        [ 0.2201,  0.0296, -0.1103],\n",
      "        [ 0.2055,  0.0368, -0.1366]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "example_model = ExampleModel()\n",
    "input_tensor = torch.rand(5, 2)\n",
    "output = example_model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "from torch.distributions.kl import kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categorical(logits: torch.Size([5, 3]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without using softmax\n",
    "dist = Categorical(logits=output)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3765, 0.3579, 0.2655],\n",
       "        [0.3819, 0.3490, 0.2691],\n",
       "        [0.3805, 0.3501, 0.2694],\n",
       "        [0.3929, 0.3248, 0.2824],\n",
       "        [0.3914, 0.3306, 0.2780]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3765, 0.3579, 0.2655],\n",
       "        [0.3819, 0.3490, 0.2691],\n",
       "        [0.3805, 0.3501, 0.2694],\n",
       "        [0.3929, 0.3248, 0.2823],\n",
       "        [0.3914, 0.3306, 0.2780]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as softmax\n",
    "e_x = torch.exp(output) \n",
    "e_x / torch.sum(e_x, dim=1, keepdim=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling using the prob dist\n",
    "dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcuate KL-Divergence\n",
    "dist_1 = Categorical(logits=output[0])\n",
    "dist_2 = Categorical(logits=output[1])\n",
    "kl_divergence(dist_1, dist_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `detach()` on long-term Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "example_model = ExampleModel()\n",
    "data_batches = [torch.rand(10, 2) for _ in range(5)]\n",
    "criterion = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.5188, grad_fn=<MseLossBackward0>), tensor(0.4238, grad_fn=<MseLossBackward0>), tensor(0.4574, grad_fn=<MseLossBackward0>), tensor(0.3861, grad_fn=<MseLossBackward0>), tensor(0.4921, grad_fn=<MseLossBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "# Bad Example\n",
    "losses = []\n",
    "# Training loop\n",
    "for batch in data_batches:\n",
    "    output = example_model(batch)\n",
    "    target = torch.rand((10, 3))\n",
    "    loss = criterion(output, target)\n",
    "    losses.append(loss)\n",
    "    # Optimization code here\n",
    "print(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the gradients for each loss is tracked and unnecessary. Could cause memory leaks!\n",
    "\n",
    "Use:\n",
    "- detach() - \"detaches\" the tensor from the computation graph and stores only the `tensor`\n",
    "- item() - extracts the scalar value from the tensor and stores it (not the tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.4588), tensor(0.4431), tensor(0.3487), tensor(0.4100), tensor(0.4622)]\n",
      "[0.458821564912796, 0.44308847188949585, 0.3487340211868286, 0.4100143015384674, 0.4622393548488617]\n"
     ]
    }
   ],
   "source": [
    "# Better way\n",
    "losses_detach = []\n",
    "losses_item = []\n",
    "\n",
    "# Training loop\n",
    "for batch in data_batches:\n",
    "    output = example_model(batch)\n",
    "    target = torch.rand((10, 3))\n",
    "    loss = criterion(output, target)\n",
    "    losses_detach.append(loss.detach())\n",
    "    losses_item.append(loss.item())\n",
    "    # Optimization code here\n",
    "print(losses_detach)\n",
    "print(losses_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  24724 KiB |  32768 KiB |   3216 MiB |   3192 MiB |\n",
      "|       from large pool |  24704 KiB |  32768 KiB |   3208 MiB |   3184 MiB |\n",
      "|       from small pool |     20 KiB |   1084 KiB |      8 MiB |      8 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  24724 KiB |  32768 KiB |   3216 MiB |   3192 MiB |\n",
      "|       from large pool |  24704 KiB |  32768 KiB |   3208 MiB |   3184 MiB |\n",
      "|       from small pool |     20 KiB |   1084 KiB |      8 MiB |      8 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  24333 KiB |  32000 KiB |   3141 MiB |   3117 MiB |\n",
      "|       from large pool |  24320 KiB |  32000 KiB |   3133 MiB |   3109 MiB |\n",
      "|       from small pool |     13 KiB |   1076 KiB |      8 MiB |      8 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  34816 KiB |  34816 KiB |  34816 KiB |      0 B   |\n",
      "|       from large pool |  32768 KiB |  32768 KiB |  32768 KiB |      0 B   |\n",
      "|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  10092 KiB |  10102 KiB |  18389 KiB |   8297 KiB |\n",
      "|       from large pool |   8064 KiB |   8064 KiB |   8064 KiB |      0 KiB |\n",
      "|       from small pool |   2028 KiB |   2047 KiB |  10325 KiB |   8297 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      23    |      31    |     244    |     221    |\n",
      "|       from large pool |       2    |       2    |     201    |     199    |\n",
      "|       from small pool |      21    |      29    |      43    |      22    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      23    |      31    |     244    |     221    |\n",
      "|       from large pool |       2    |       2    |     201    |     199    |\n",
      "|       from small pool |      21    |      29    |      43    |      22    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       3    |       3    |       3    |       0    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       2    |       3    |       9    |       7    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       1    |       2    |       8    |       7    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Memory allocated: 24724 KB\n",
      "Memory cached: 34816 KB\n",
      "Memory cached: 34816 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SINJOY\\AppData\\Local\\Temp\\ipykernel_224776\\243746751.py:4: FutureWarning: `torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`\n",
      "  print(f\"Memory cached: {torch.cuda.memory_cached() // 1024} KB\")\n"
     ]
    }
   ],
   "source": [
    "# Built-in torch funcs\n",
    "print(torch.cuda.memory_summary())\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() // 1024} KB\")\n",
    "print(f\"Memory cached: {torch.cuda.memory_cached() // 1024} KB\")\n",
    "print(f\"Memory cached: {torch.cuda.memory_reserved() // 1024} KB\") # memory_cached renamed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trick to Delete a Model from GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats():\n",
    "    print(f\"Memory allocated: {torch.cuda.memory_allocated() // 1024} KB\")\n",
    "    print(f\"Memory cached: {torch.cuda.memory_reserved() // 1024} KB\") # pt rename memory_cached -> memory_reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 24727 KB\n",
      "Memory cached: 34816 KB\n",
      "Memory allocated: 24724 KB\n",
      "Memory cached: 34816 KB\n"
     ]
    }
   ],
   "source": [
    "example_model = ExampleModel().cuda()\n",
    "print_stats()\n",
    "del example_model\n",
    "print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache is not deleted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n",
      "Memory allocated: 24727 KB\n",
      "Memory cached: 34816 KB\n",
      "\n",
      "Model deleted\n",
      "Memory allocated: 24724 KB\n",
      "Memory cached: 34816 KB\n",
      "\n",
      "GC\n",
      "Memory allocated: 24724 KB\n",
      "Memory cached: 34816 KB\n",
      "\n",
      "Emptied cache!\n",
      "Memory allocated: 24724 KB\n",
      "Memory cached: 34816 KB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "example_model = ExampleModel().cuda()\n",
    "print(\"Model initialized\")\n",
    "print_stats()\n",
    "\n",
    "del example_model\n",
    "print(\"\\nModel deleted\")\n",
    "print_stats()\n",
    "\n",
    "gc.collect() # good practice\n",
    "print(\"\\nGC\")\n",
    "print_stats()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nEmptied cache!\")\n",
    "print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call `eval()` before Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExampleModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model = ExampleModel()\n",
    "\n",
    "# Training mode\n",
    "example_model.train()\n",
    "\n",
    "# Testing mode\n",
    "example_model.eval() # OR example_model.train(mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: \n",
    "\n",
    "`model.eval()` notifies all layers - affects layers like InstanceNorm, BatchNorm, Dropout (since they behave differently in training and testing phases)\n",
    "\n",
    "`with torch.no_grad():` affects only the autograd engine - deactivates it and speeds up computation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit a single batch before actual training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call `.zero_grad()` before `.backward()` (but only once before gradient accumalation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't use `Softmax` when using `CrossEntropy`\n",
    "\n",
    "Already uses it inside the PyTorch loss function implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias term not need with BatchNorm (just unneccessary, nothing breaks!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `View` vs `Permute`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Careful when applying Data Augmentation\n",
    "\n",
    "- Flipping and rotating (180 degs) in digit dataset - check `6` vs `9` confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the Data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient (Norm) Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
